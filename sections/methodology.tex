\section{Data and methodology}\label{sec:methodology}

This section presents the proposed data sources and methodology of the thesis. \S\ref{subsec:data-sources} introduces the Universal Dependencies treebanks as well as additional resources that will be used as reference and validation in this study. The rest of the section, \S\ref{subsec:features}-\ref{subsec:infotheory}, presents the main computational methods to be used in the thesis.

\subsection{Data sources}\label{subsec:data-sources}

Universal Dependencies (UD) is designed to be a cross-linguistically consistent system for annotating morphosyntactic information within a dependency grammar framework \citep{demarneffe2021}. The UD treebanks \citep{universaldep} is the collection of cross-lingual treebanks annotated in the UD framework by an open community of more than 300 contributors. See \ref{tab:treebanks} for a table of languages available in UD v2.5, as an example. \todo{include some more statistics of UD and a potential list of languages to study}

\input{figures/ud_languages.tex}

A subset of the UD treebanks, the Parallel Universal Dependencies (PUD) treebanks were originally developed for the CoNLL-2017 Shared Task \citep{zeman2017} and include 1000 sentences in 18 languages that were randomly picked from newswire and Wikipedia and annotated according to UD v2 guidelines. The 18 languages are English, German, French, Italian, Spanish, Arabic, Hindi, Chinese, Indonesian, Japanese, Korean, Portuguese, Russian, Thai, Turkish, Czech, Finnish and Swedish. Of the sentences, 750 were originally English, while the remaining 250 sentences come from German, French, Italian and Spanish texts and translated to other languages through English. While facing obvious limitation in terms of language coverage, corpus size, and possible artifacts due to the so-called ``translationese'', parallel corpora allow for cross-lingual comparison with a smaller data size and will also be considered in this thesis.

In additional to the main data source of UD treebanks, additional resources will be used in the study as reference and to perform validation and evaluation of the intermediate results. As an example, the valency frames and verb classes as induced from the UD treebanks will be validated, where possible, against the expert-annotated data from \textbf{ValPaL}\citep{valpal}. \todo{introduce ValPaL a bit more, mention VerbNet, PropBank, FrameNet as necessary}

\subsection{Verb valency features}\label{subsec:features}

In the first step, the specific uses of verbs are abstracted through a feature selection process. Each instance of verb use will be represented by syntactic features of the sentence, namely only features that are considered part of valency frame encoding are included. This is in order to focus on whether semantically coherent verb classes can be induced on valency frame information alone. In selecting the features, cross-lingual differences in valency frame coding will be taken into account, e.g. whether a language uses morphological cases or word order to encode valency frame information. \todo{Examples from EN, DE, ZH} 


\subsection{Clustering}\label{subsec:clustering}

The clustering process after feature selection consists of two steps, but the clustering algorithms used need not be the same. The first is the automatic induction of valency frames in a language given the selected features. Given the valency frames in a language, each verb will then be represented by its distribution over the valency frames, which are then clustered into the verb classes. Since we intend to perform unsupervised clustering, the number of valency frames and the number of verb classes are not known beforehand. This requires either using of algorithms that do not require a predefined number of clusters (e.g. Ward clustering), or experimenting with cluster sizes with each language (cf. \cite{schulteimwalde2006}, which used the k-means algorithm with a predefined the gold standard number). Due to the lack the gold standard for many of the languages to be experimented on, the latter seems preferable. The agglomerative clustering method will be used.

Given the relative low dimensionality of hand-selected features, complex clustering algorithms are not anticipated to be necessary. Nevertheless, other clustering algorithms should also be investigated \citep{xu2015a}. Given the two levels of clustering, an approach to be explored is the Hierarchical Dirichlet process, which is particularly suited for clustering grouped data (cf. \citet{parisien2010}, Fig.~\ref{fig:parisien2010}, where a Hierarchical Dirichlet process was extended to account for diathesis alternations).

\begin{figure}
    \includegraphics[width=\textwidth]{figures/verb_alternation_classes.png}
    \centering
    \caption{(a) Model 1, a Hierarchical Dirichlet Process applied to learning verb argument structure constructions. (b) Model 2,
    an extension of Model 1 to learn verb alternation classes.}\label{fig:parisien2010}
\end{figure}

\subsection{Cross-lingual verb sense alignment}\label{subsec:verblist}

A cross-lingual aligned list of counterpart verbs will be needed to compare the verb classes and valency frames. The easiest way to do this is likely through existing cross-lingual word lists such as LanguageNet, part of the PanLex project. http://uakari.ling.washington.edu/languagenet/available/

Alternatively, lexicon induction from cross-lingual word embeddings and other NLP methods may also be considered.

\subsection{Information theory}\label{subsec:infotheory}

Complexity and point-wise mutual information, cf. in \citet{say2014}.

Complexity \& Point-wise mutual information (PMI)